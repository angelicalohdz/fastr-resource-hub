---
marp: true
theme: fastr
paginate: true
---


# FASTR Workshop - TestCountry

**January 15-17, 2026** | **TestCity**

*Test Facilitator*

<img src="../resources/logos/FASTR_Primary_01_FullName.png" style="position: absolute; bottom: 40px; right: 40px; width: 180px;">

---


<!-- _class: agenda -->
# Workshop Agenda - Day 1

| Time | Session | Speaker |
|------|--------|--------|
| 8:30 AM - 9:00 AM | **Registration** |  |
| 9:00 AM - 9:15 AM | **Welcome & Opening Remarks** |  |
| 9:15 AM - 9:45 AM | **Participant Introductions** |  |
| 9:45 AM - 10:00 AM | **Workshop Objectives** |  |
| 10:00 AM - 10:20 AM | **Country Overview** |  |
| 10:20 AM - 10:35 AM | **Health Priorities** |  |
| 10:35 AM - 10:50 AM | *Tea Break* | |
| 10:50 AM - 12:05 PM | **Introduction to the FASTR Approach** |  |
| 12:05 PM - 12:30 PM | **Identify Questions & Indicators** |  |
| 12:30 PM - 1:30 PM | *Lunch* | |
| 1:30 PM - 2:30 PM | **Identify Questions & Indicators** |  |
| 2:30 PM - 3:30 PM | **Identify Questions & Indicators** |  |
| 3:30 PM - 3:45 PM | *Afternoon Tea* | |
| 3:45 PM - 4:20 PM | **Identify Questions & Indicators** |  |
| 4:20 PM - 4:45 PM | **Action Planning** |  |
| 4:45 PM - 5:00 PM | **Day Wrap-up & Q&A** |  |

---

<!-- _class: agenda -->
# Workshop Agenda - Day 2

| Time | Session | Speaker |
|------|--------|--------|
| 9:00 AM - 9:15 AM | **Recap & Questions** |  |
| 9:15 AM - 10:15 AM | **Data Extraction** |  |
| 10:15 AM - 10:30 AM | **Data Extraction** |  |
| 10:30 AM - 10:45 AM | *Tea Break* | |
| 10:45 AM - 11:00 AM | **Data Extraction** |  |
| 11:00 AM - 12:00 PM | **The FASTR Data Analytics Platform** |  |
| 12:00 PM - 12:30 PM | **The FASTR Data Analytics Platform** |  |
| 12:30 PM - 1:30 PM | *Lunch* | |
| 1:30 PM - 2:00 PM | **The FASTR Data Analytics Platform** |  |
| 2:00 PM - 3:30 PM | **Group Work & Discussion** |  |
| 3:30 PM - 3:45 PM | *Afternoon Tea* | |
| 3:45 PM - 4:45 PM | **Group Work & Discussion** |  |
| 4:45 PM - 5:00 PM | **Day Wrap-up & Q&A** |  |

---

<!-- _class: agenda -->
# Workshop Agenda - Day 3

| Time | Session | Speaker |
|------|--------|--------|
| 9:00 AM - 9:15 AM | **Recap & Questions** |  |
| 9:15 AM - 10:30 AM | **Data Quality Assessment** |  |
| 10:30 AM - 10:45 AM | *Tea Break* | |
| 10:45 AM - 11:45 AM | **Data Quality Adjustment** |  |
| 11:45 AM - 12:30 PM | **Hands-on Practice: DQ Adjust** |  |
| 12:30 PM - 1:30 PM | *Lunch* | |
| 1:30 PM - 3:30 PM | **Group Work & Discussion** |  |
| 3:30 PM - 3:45 PM | *Afternoon Tea* | |
| 3:45 PM - 4:45 PM | **Group Work & Discussion** |  |
| 4:45 PM - 5:05 PM | **Next Steps & Action Planning** |  |
| 5:05 PM - 5:20 PM | **Day Wrap-up & Q&A** |  |

---



# Workshop Objectives

By the end of this workshop, participants will be able to:

- Understand the FASTR approach to routine data analysis
- Learn to assess and adjust for data quality issues
- Apply methods to analyze service utilization and coverage
- Practice interpreting and communicating results

---



# Country Health System Overview

**TestCity**

---

## Health System Structure

| Level | Description |
|-------|-------------|
| National | [Ministry of Health] |
| Regional | [X provinces/regions] |
| District | [X districts] |
| Facility | 5000 health facilities |

**Reporting to DHIS2:** 4500 (90%)

---

## Population

| Group | Estimate |
|-------|----------|
| Total population | 50000000 |
| Women of reproductive age | 12000000 |
| Children under 5 | 8000000 |
| Expected pregnancies/year | 1500000 |
| Expected live births/year | 1400000 |

---

## Data Sources

**Routine data:**
- DHIS2 (reporting rate: 90%)

**Survey data:**
- DHS 2023

---



# Health Priorities

## Focus areas for TestCountry

- Improve data quality in RMNCAH indicators
- Strengthen routine data use

---



## Introduction to FASTR

<div class="columns">
<div>

The Global Financing Facility (GFF) supports country-led efforts to strengthen the use of timely data for decision-making, with the goal of improving primary healthcare (PHC) performance and RMNCAH-N outcomes.

**Frequent Assessments and Health System Tools for Resilience (FASTR)** is the GFF's rapid-cycle analytics framework for monitoring health system performance using high-frequency data.

FASTR brings together four complementary technical approaches:

1. Routine HMIS data analysis
2. Health facility phone surveys
3. High-frequency household phone surveys
4. Follow-on, problem-driven analyses

</div>
<div>

![FASTR Technical Approaches](../resources/diagrams/Technical-Rapid-cycle-analytics--V3.svg)

</div>
</div>

---



## What FASTR does with routine HMIS data

<div class="columns">
<div>

FASTR works directly with Ministries of Health to transform routine HMIS data into actionable evidence for policy and program management.

Using facility-level data, the approach focuses on three core analytic functions:

**Assess data quality**
Identify key issues related to completeness, outliers, and internal consistency.

**Adjust for data quality limitations**
Apply transparent, indicator-specific methods to improve the reliability of trend analysis.

**Analyze service use and coverage trends**
Track changes in priority RMNCAH-N services and compare progress against country priorities and benchmarks.

</div>
<div>

![Steps to implement RMNCAH-N service use monitoring](../resources/diagrams/Steps%20to%20implement%20RMNCAH-N%20service%20chart.svg)

</div>
</div>

---



## Why rapid-cycle analytics?

<div class="columns">
<div>

Routine health information systems are a critical source of data, but they are often underused due to concerns about data quality and long delays between data collection and analysis. Traditional household and facility surveys, while essential, are resource-intensive and infrequent.

FASTR's rapid-cycle analytics address this gap by providing:

- Timely insights aligned with country decision cycles
- Continuous learning rather than one-off assessments
- Direct feedback loops between data, analysis, and action

During the COVID-19 pandemic, this approach was applied in over 20 countries to monitor disruptions to essential RMNCAH-N services and inform response and recovery planning.

</div>
<div>

![FASTR rapid-cycle analytics framework](../resources/diagrams/GFF-Rapid-Cycle-Analytics-Data-Use_Figure-1.svg)

</div>
</div>

---



## Focus of the analysis

### Core indicators

FASTR prioritizes a core set of RMNCAH-N indicators that:

- Represent key service delivery contacts across the continuum of care
- Have relatively high reporting completeness and volumes
- Serve as proxies for broader service delivery performance

Outpatient consultations are included as a proxy for overall health service use. The indicator set can be expanded to reflect country-specific priorities.

### Core data quality metrics

Analysis is anchored in a standardized set of data quality metrics, including:

- Reporting completeness
- Extreme value (outlier) detection
- Consistency across related indicators

These metrics are summarized into an overall data quality score to support interpretation and comparison across areas.

---



## FASTR approach to routine data analysis

The FASTR approach follows a three-step workflow:

### 1. Assess data quality
Identify issues related to completeness, outliers, and internal consistency at national and subnational levels.

### 2. Adjust for data quality limitations
Apply transparent, indicator-specific corrections to improve the reliability of trend analysis.

### 3. Analyze service delivery
Quantify changes in priority service volumes and compare coverage trends against country targets.

This enables continuous, subnational monitoring while data quality is systematically improved.

---


# Tea Break

**15 minutes**

We'll resume at 10:30 AM

---


## Introduction to FASTR: gaps and challenges

*Content to be developed*

This section will cover:
- Identifying gaps and challenges that FASTR is well suited to support
- How FASTR serves as an entry point to reduce fragmentation
- Starting the conversation with government stakeholders

---



## Development of a data use case

*Content to be developed*

This section will cover:
- Co-creation workshop approach with MoH and stakeholders
- Data use case development guidance
- Example use cases from country implementations

---



## Defining priority questions

Effective data use relies on well-defined questions. Priority questions will guide the FASTR analysis and enhance decision making support.

**Qualities of a good question:**

- **Addresses a priority issue**: A topic of interest to you and policy makers
- **Relevant**: Important enough to be worth answering
- **Related to experiences that are alive**: Connected to current issues
- **Important to individuals/groups**: Matters to stakeholders
- **Answerable**: Can be addressed with available data and methods

---



## Is my question a relevant priority? 5+ Ws to consider

- **Who** is your audience?
- **What** do they need and want to know?
- **When** do they need to know it by?
- **When** is the event/intervention/period they are interested in?
- **Why** do they need to know?
- **How** will they use the findings?

---



## What do we mean by answerable?

**We have the data**
- Type, quantity, quality sufficient for the question

**We have the analysis tools/methods**
- Statistically valid; feasible to use

**We have the time**
- We can answer the question on a quarterly basis

---



<!-- Note: This slide was hidden in the original presentation but may be useful to include -->
## PICO framework for identifying answerable questions

A standard tool from evidence-based medicine and public health research for formulating clear, answerable questions.

| Component | Description |
|-----------|-------------|
| **P**opulation | Who is being investigated |
| **I**ntervention | What is being investigated |
| **C**omparison | What is baseline/non-intervention |
| **O**utcome | What is public health objective |

---



## What makes a good indicator for FASTR analysis?

- **Relevance**: Does this indicator align with our priority questions and objectives?
- **Volume**: Is this indicator collected at a high volume, improving robustness of analysis?
- **Completeness**: Does the indicator have a high completeness rate across reporting facilities?
- **Frequency**: Is the indicator reported frequently enough (e.g., monthly) to support rapid-cycle analysis?
- **Type**: Is this indicator a count of services delivered?

---



## Why focus on high volume indicators?

One key added-value of the FASTR approach is making adjustments for data quality. Low-volume indicators are challenging to adjust:

- **Greater sensitivity to outliers**: A single unusually high or low data point can disproportionately impact overall analysis
- **Unstable estimates**: Small variations can lead to large percentage changes, making it harder to distinguish genuine trends from random variability
- **Difficulty identifying true outliers**: Challenging to determine whether a data point is genuinely an outlier or part of natural variability

Count indicators allow for ongoing data quality checks with improved precision in identifying outliers.

---



## Why focus on high completeness indicators?

High completeness indicators improve data quality, reduce bias, and enable more accurate insights:

- **Data reliability**: Data is representative of the full picture across facilities, regions, or populations
- **Consistency in analysis**: Data points from most or all reporting units allow for consistent analysis across time periods and locations
- **Reduced risk of misinterpretation**: Incomplete data can lead to incorrect conclusions (e.g., low completeness might falsely suggest a drop in service utilization)

Statistical methods such as imputation can adjust for incomplete data, but this requires assumptions about the missing data.

---



## Why focus on count indicators?

**Challenges with proportion indicators:**
- Proportions limit our ability to apply adjustments for data quality challenges
- Numerators may have data quality challenges, skewing actual coverage levels
- Denominators may be outdated or inaccurate
- Using numerators and denominators separately allows adjustments to both

**Mortality as a rare event:**
- Mortality indicators are inherently low-frequency, making proportional adjustments unreliable
- Better suited for annual reviews than monthly or quarterly updates

---



## FASTR core indicators

The FASTR approach focuses on a core set of RMNCAH-N indicators that:
- Characterize the reproductive, maternal and child healthcare continuum
- Capture key service delivery events with higher completeness rates and higher volume
- Serve as proxies for other services delivered at the same contact

Outpatient consultations (OPDs) are used as a proxy for general use of health services.

Additional country and program-specific indicators can be added to be responsive to country priorities.

---



## Preparing for data extraction

*Content to be developed*

This section will cover:
- Pre-extraction checklist
- Understanding your DHIS2 configuration
- Mapping indicators to data elements
- Planning your extraction timeline

---



# See You Tomorrow!

## Day 1 Complete

We covered today:

- Introduction to the FASTR Approach
- Identify Questions & Indicators

---

## Tomorrow: Day 2

We resume at **9:00 AM**

**Coming up:** Data Extraction, The FASTR Data Analytics Platform

---



# Day 2: Recap & Questions

## Yesterday we covered:

- Introduction to the FASTR Approach
- Identify Questions & Indicators

---

## Questions & Discussion

- Any questions from yesterday's sessions?
- Points that need clarification?
- Insights from the exercises?

---



## Why extract data from DHIS2?

### Data quality adjustment

The FASTR approach focuses on data quality adjustments to expand the analyses countries can do with DHIS2 data and to generate more robust estimates.

The FASTR methodology includes specific approaches to:
- Identify and adjust for outliers
- Adjust for incomplete reporting
- Apply consistent data quality metrics

These adjustments require processing that cannot be done within DHIS2's native analytics.

---



## Tools for data extraction

*Content to be developed*

This section will cover:
- DHIS2 data export options
- API-based extraction methods
- Data transformation requirements
- Quality checks on extracted data

---


# Lunch Break

**60 minutes**

We'll resume at 12:30 PM

---


## Overview of the platform

The FASTR analytics platform is a web-based tool designed to support data quality assessment, adjustment, and analysis for routine health data.

It allows users to upload and analyze data from various sources, including DHIS2, with built-in statistical methods to generate an adjusted dataset and run priority analyses on selected indicators.

The platform provides a user-friendly interface for running analyses and offers flexible options for visualizing and exporting results.

---



## Accessing the platform

*Content to be developed*

This section will cover:
- Creating accounts
- Signing in
- User permissions and roles

---



## Setting up the structure

*Content to be developed*

This section will cover:
- Configuring admin areas (regions, districts)
- Setting up facilities
- Defining indicators

---



## Importing a dataset

*Content to be developed*

This section will cover:
- Data format requirements
- Import process
- Validation and error handling

---



## Installing and running modules

*Content to be developed*

This section will cover:
- Available analysis modules
- Module installation
- Running analyses

---



## Creating a new project

*Content to be developed*

This section will cover:
- Project setup workflow
- Configuration options
- Best practices

---



## Creating visualizations

*Content to be developed*

This section will cover:
- Available chart types
- Customization options
- Exporting visualizations

---



## Creating reports

*Content to be developed*

This section will cover:
- Report templates
- Automated report generation
- Customizing reports

---



# See You Tomorrow!

## Day 2 Complete

We covered today:

- Data Extraction
- The FASTR Data Analytics Platform

---

## Tomorrow: Day 3

We resume at **9:00 AM**

**Coming up:** Data Quality Assessment, Data Quality Adjustment

---



# Day 3: Recap & Questions

## Yesterday we covered:

- Data Extraction
- The FASTR Data Analytics Platform

---

## Questions & Discussion

- Any questions from yesterday's sessions?
- Points that need clarification?
- Insights from the exercises?

---



## Data quality assessment

Understanding the reliability of routine health data

---
## Why talk about data quality?

**The challenge:** Health facilities report data every month, but sometimes:
- Numbers seem too high or too low
- Facilities forget to report
- Related numbers don't match up

**The impact:** Bad data leads to bad decisions
- We might think services are improving when they're not
- We might miss real problems in certain areas
- Resources might go to the wrong places

**FASTR's solution:** Check data quality systematically, fix what we can, and be transparent about limitations

---
## Three simple questions about data quality

**1. Are facilities reporting regularly?**
- Completeness: Did we get reports from facilities this month?

**2. Are the numbers reasonable?**
- Outliers: Are there any suspiciously high values?

**3. Do related numbers make sense together?**
- Consistency: Do related services show expected patterns?

These three questions help us understand if we can trust the data for decision-making.

---



## Question 1: Are facilities reporting?

---

## Completeness: Did we get reports?

**What we're checking:**
Each month, are facilities sending in their reports?

**Example:**
- District has 20 health centers
- In March, only 15 sent ANC data
- **Completeness = 75%** (15 out of 20 reported)

**Why it matters:**
- If many facilities don't report, we're missing part of the picture
- Trends might look like services dropped, when really facilities just didn't report

---

## What's good completeness?

**It depends on your health system:**
- 90%+ is excellent
- 80-90% is good
- Below 80% means we're missing a lot of information

**Important:** Even 100% completeness doesn't mean we have the full picture - some services might happen outside facilities or some facilities might not be in the reporting system.

**What to look for:** Is completeness improving over time? Which areas have low completeness?

---

## Completeness: FASTR output

![Indicator Completeness](../resources/default_outputs/Default_2._Proportion_of_completed_records.png)

---



## Question 2: Are numbers reasonable?

---

## Outliers: Spotting suspicious numbers

**What we're checking:**
Are there any values that seem way too high compared to what that facility normally reports?

**Real example:**
- Health Center A normally reports 20-25 deliveries per month
- In March, they reported 450 deliveries
- **This is likely a data entry error** (maybe they typed an extra digit, or reported cumulative instead of monthly)

**Why it matters:**
- One extreme value can make it look like there was a huge service increase
- Skews totals and trends for the whole district or province

---

## How we spot outliers

Outliers are identified by assessing the within-facility variation in monthly reporting for each indicator.

A value is flagged as an outlier if it meets EITHER of two criteria:

1. A value greater than 10 times the Median Absolute Deviation (MAD) from the monthly median value for the indicator, OR
2. A value for which the proportional contribution in volume for a facility, indicator, and time period is greater than 80%

AND for which the count is greater than 100.

---

## Outlier example

**Health Center B - Malaria tests:**

| Month | Tests Reported | Normal? |
|-------|----------------|---------|
| January | 245 | Normal |
| February | 267 | Normal |
| **March** | **2,890** | **Outlier** |
| April | 256 | Normal |

**What happened?** Probably someone entered "2890" instead of "289" (extra zero)

**Impact if we don't fix it:** March would show a huge "spike" in malaria that didn't really happen.

---

## Outliers: FASTR output

![Outliers](../resources/default_outputs/Default_1._Proportion_of_outliers.png)

---



## Question 3: Do related numbers match up?

---

## Consistency: Do related services make sense together?

**What we're checking:**
Health services are related - certain patterns are expected.

**Example 1 - ANC visits:**
- More women should get their **1st** ANC visit (ANC1)
- Fewer should complete all **4** visits (ANC4)
- We expect: ANC1 >= ANC4

**Example 2 - Vaccinations:**
- More babies should get their **1st** Penta dose (Penta1)
- Fewer should complete all **3** doses (Penta3)
- We expect: Penta1 >= Penta3

**If these relationships are backwards, something's wrong with the data.**

---

## Why check consistency at district level?

**Patients move between facilities:**
- Woman might get ANC1 at Health Center A
- But deliver at District Hospital B
- If we only look at each facility separately, numbers might not match

**Solution:** Check consistency at district level
- Add up all ANC1 visits in the district
- Add up all ANC4 visits in the district
- Compare the totals

This accounts for patients visiting different facilities for different services.

---

## Consistency example

**District X - ANC Services:**

| Indicator | District Total | Expected Relationship |
|-----------|----------------|----------------------|
| ANC1 | 5,200 visits | Should be higher |
| ANC4 | 4,100 visits | Should be lower |

**This passes the consistency check** - more women started ANC (5,200) than completed 4 visits (4,100).

**If it was reversed** (more ANC4 than ANC1), we'd know there's a data quality problem.

---

## Consistency: FASTR output

![Internal Consistency](../resources/default_outputs/Default_4._Proportion_of_sub-national_areas_meeting_consistency_criteria.png)

---



## Putting it all together: Overall data quality

---

## Overall quality score

**For each facility and month, we combine all three checks:**

**Complete:** Did the facility report?
**No outliers:** Are the numbers reasonable?
**Consistent:** Do related numbers make sense?

**Binary DQA Score:**
- dqa_score = 1 if all consistency pairs pass
- dqa_score = 0 if any consistency pair fails

**DQA Mean:** Average of completeness-outlier score and consistency score

**This score helps us:**
- Decide which data to use for analysis
- Identify facilities that need support
- Track if data quality is improving over time

---

## Overall DQA score: FASTR output

![Overall DQA Score](../resources/default_outputs/Default_5._Overall_DQA_score.png)

---

## Mean DQA score: FASTR output

![Mean DQA Score](../resources/default_outputs/Default_6._Mean_DQA_score.png)

---


# Afternoon Break

**15 minutes**

We'll resume at 3:30 PM

---

# See You Tomorrow!

**Day 3 Complete**

We resume tomorrow at **9:00 AM**

---


## Approach to data quality adjustment

The Data Quality Adjustment module (Module 2 in the FASTR analytics platform) systematically corrects two common problems in routine health facility data:

1. **Outliers** - extreme values caused by reporting errors or data entry mistakes
2. **Missing data** - from incomplete reporting

Rather than simply deleting problematic data, this module replaces questionable values with statistically sound estimates based on each facility's own historical patterns.

---

### Four adjustment scenarios

The module produces four parallel versions of the data:

| Scenario | Description |
|----------|-------------|
| **None** | Original data, no adjustments |
| **Outliers only** | Only outlier corrections applied |
| **Completeness only** | Only missing data filled in |
| **Both** | Both types of corrections applied |

This allows analysts to understand how sensitive their results are to different data quality assumptions.

---



## Adjustment for outliers

For each value flagged as an outlier, the module calculates what the value "should have been" based on that facility's historical pattern.

**Methods used (in order of preference):**
1. Centered 6-month rolling average (3 months before + 3 months after)
2. Forward 6-month rolling average
3. Backward 6-month rolling average
4. Same month from the previous year (for seasonal indicators)
5. Facility-specific historical mean (fallback)

---

### Outlier adjustment: FASTR output

![Percent change in volume due to outlier adjustment.](../resources/default_outputs/Default_1._Percent_change_in_volume_due_to_outlier_adjustment.png)

Heatmap showing percent change in service volumes due to outlier replacement.

---



## Adjustment for completeness

For months where data is missing or marked as incomplete, the module imputes (fills in) values using the same rolling average approach.

This ensures that temporary reporting gaps don't create artificial drops to zero in the data.

---

### Completeness adjustment: FASTR output

![Percent change in volume due to completeness adjustment.](../resources/default_outputs/Default_2._Percent_change_in_volume_due_to_completeness_adjustment.png)

Heatmap showing percent change in service volumes due to missing data imputation.

---


# Tea Break

**15 minutes**

We'll resume at 10:30 AM

---

# See You Tomorrow!

**Day 3 Complete**

We resume tomorrow at **9:00 AM**

---


# Next Steps

## Action items

- Maternal health
- Child nutrition

---

## Contact

For questions or support:
- **Email:** fastr@worldbank.org
- **Resources:** [FASTR Resource Hub](https://fastr-analytics.github.io/fastr-resource-hub/)

---


# Thank You!

## Questions & Discussion

---

# Contact Information

**FASTR Team**

üìß Email: fastr@worldbank.org
üåê Website: https://fastr.org

---

